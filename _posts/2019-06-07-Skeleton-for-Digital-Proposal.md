---
title: DRAFT How to Write a Product Proposal 
category: "Digital Transformation"
tag: ["Template", "Product", "Letter of Proposal"]
toc: true
---

The primary role of a product proposal document is to get funding from a sponsor, regardless of who the sponsor is. Whether you are a new product manager at a startup or a junior partner consultant seeking the business of a large enterprise (in the middle of a [digital transformation](https://blog.dannycastonguay.com/digital%20transformation/Digital-Transformations-Org-Changes/)), chances are that you are struggling with getting your product proposal accepted. I certainly did when I played such roles before, and quite frankly I still do today.

With the mindset that a proposal document is a product, whose goal is to discover, persuade and influence others to join a cause, it is in itself a worthwhile exercise for the author (presumably the de facto product manager).

I propose here a framework for how to organize a product proposal, as well as a few sample paragraphs which I have used before for each sections. I'm hoping that the framework and these examples will help or inspire entrepreneurs, consultants and product managers out there - please don't be shy and send me your thoughts and feedback.

# The PRODUCT Proposal Framework

The framework is organized in order of priority for the sponsor, who presumably will also have to convince others to deploy the capital to build this product. 

It may be a bit cheeky, but I found the word PRODUCT to be a memorable acronym for the framework.

## Framework structure

Here's how the overall framework structure is organized:

1. **P**ersonalized letter
2. **R**undown (i.e., executive summary)
3. **O**utline of work
4. **D**esign and user insights
5. **U**nderstanding of implementation challenges   
6. **C**ost estimates
7. **T**ools and processes

## Framework explained

The **Personalized letter** serves as a way to tie the context of the last and next conversations you plan on having with the sponsor. This introductory part of the document should be short, and deal more with building a trusted relationship than deep content. 

The **Rundown** serves as a single page document that is broad in scope, but laser sharp on the important details. Think of it as the bullet points that you want your counterpart to use to go persuade a boss, or a board/advisors. It is different from an abstract because those tend to be very short (and academic), and it is also different from an Executive Summary because those tend to focus more on the financial aspect. I want to avoid focussing on the financial upside because it is premature optimization ([the root of all evil](http://www.paulgraham.com/knuth.html)) - in time, it will come if you build something valuable that people want. 

The **Outline of work** gives a sense of the sequence of steps you plan on taking to deliver the first few months of work. Even if you are a startup and don't know exactly where you are going, it is a good idea to spend at least a few hours initially (and periodically after) to think carefully about sequencing important pieces of work. Agile or not, anticipating the future is smart, as long as you don't fool yourself into thinking that you have perfect foresight and avoid the trap of thinking in terms of "late" or "missed deadlines".

The **Design and user insights** comes next and gives the reader reassurance that you understand the pain points you are solving to create a great User Experience (UX) that will drive desired benefits (revenue increase, cost decrease, risk mitigation or compliance). The section is in the middle of the framework for a reason - it should be the heart of the document, written with the most passion. 

The **Understanding of implementation challenges** is a necessary section only in so far that you do not yet have complete trust from your sponsor in your ability to deliver good software products, or that there is uncertainty on what's possible to achieve (e.g., you can't do good data science if you don't have data). In many cases it's less about you, and it's a more about the sponsor. For instance, you may have a sponsor who has never built products before, or has prior bitter experiences  (common for both VCs and large Enterprise alike). If at the end of a discussion, your sponsor seems overly focussed on implementation details (e.g., "will you use Apache Kafka?"), then expect that you need to invest more time in detailing the (anticipated) approach up-front. Regardless of the reasons, it's an important section. 

The **Costs estimates** are one of the most critical section to get to a "yes", despite its place (second to last) in the framework. Note that it's better to be upfront and transparent about costs, because it enables the sponsor/investors to do sanity checks on your ability to deliver. If your costs are too low, chances are you are under-estimating the work involved. If your costs are too high, chances are that you are trying to build too much too quickly, or you are just paying people too much. 

Finally, **Tools and processes** are a bit more mundate, but you can't really do without them, so might as well get everyone aligned on a common language up front. What issue tracker will you use? Where will the team be located? What are expectations in terms of hours? What's the process for Cybersecurity? How will defects be reported? This section is especially important in heavily regulated industries (e.g., financial services). 

# Sample content

If you have read this far, thank you! A framework alone isn't very useful so I'm giving you sanitized examples from prior proposals I've written. Please feel free to adapt to your needs.

## 1. Personalized letter

`<Date>`

Dear `<Names>`,

Thank you for the invitation to discuss the `<product idea>` opportunity with you. I want to help you achieve `<business goal>` by targetting `<customer segment>` with a brand positioning inspired by `<sample product>`.

The `<product>` can be placed in `<channel>`, promoted to people with `<incentives>` to increase adoption, and priced according to `<pricing hypothesis>`.

I've been thinking a lot about `<end user/customer>`, and realized that the most critical `<pain points>` can be addressed by the following features:

+ `<feature1: benefit1>`
+ `<feature2: benefit2>`
+ `<feature3: benefit3>`

While there are some tradeoffs to consdier, such as `<list tradeoffs>`, imagine a future where `<product vision>`.

I'm excited to work with you on this because I firmly believe in `<sponsor's company>` role to ensure that `<company purpose>`.

Looking forward to a kickoff on `<Date>`.

Thank you,

`<Your name and contact info>`

## 2. Rundown

### 2.1 Situation

The `<industry>` currently is adopting `<industry trends>`. More and more pressure (or opportunities) is coming from `<substitute products>` and new `<competitors>`. Customers now expect `<sample feature>` and have more market power than before. 

### 2.2 Complication (problem statement)

Your organization is accountable for `<business model>` to its customers. You are now positioned to capture `<opportunity size>`.

`<list the questions or most important user journey stories>`

### 2.3 Resolution 

By building `<list of features>`, we can address the customer needs `<list of benefits>`. I estimate that it will take `<a few weeks/months>` for a team of `<2-10>` people to deliver those features for a cost of `<estimate>`. 

## 3. Outline of work

Based on your feedback, I propose a proof of concept phase, followed by a three-phased approach to build the two tools, from conception to quantifiable impact. At the end of each of the four phases, a stage gate meeting will be held to review completion of the work and make a decision on the best next steps. All of this will be done in close collaboration with `<relevant teams>`.

1. *Proof of concept (POC)*
    1. Conduct Google Ventures Design Sprint.
    2. Evaluate current data sources and pipelines.
    3. Implement proof of concept solution based sample data.
2. *User experience (UX), data gathering, and baseline tool*
    1. Interview stakeholders to understand the desired end user experience.
    2. Gather, test and validate data (internal, external, and vendor).
    3. Build an prototype that provides private test users with something tangible and is useful.
    4. Validate that the prototype brings value to the end user.
3. *Feature expansion*
    1. Develop the API against which the product will connect, and setup the base processes (continuous integration/delivery, cyber security, quality assurance and testing, etc.).
    2. Implement and test web tool that connects to data in real time.
    3. Release the product to a small set of test users, expanding from the initial private users.
    4. Validate UX hypothesis formulated in Phase 1 and adjust as necessary.
    5. Put in place metric to start to quantitatively (and objectively) measure the operational impact.
4. *Finalize the product and launch*
    1. Decide on the most valuable set of features to include in V0.1 of the product
    2. Implement required processes (cyber security, compliant, documentation, quality assurance, testing, etc.)
    3. Implement, monitor, and stabilize the product.
    4. Prepare deployment, and provide real-time world class support to ensure smooth and sequenced roll out to organization.

## 4. Design

We will be crafting the User Experience alongside Engineering developments and will be just as iterative to ensure things are moving along. We want to start with a base level of clear access to needed information to make sure features are usable from the get go. We will then continually revise and improve application usability based on user feedback .To make the most of the simulation and optimization back-end, we will be considering things like easy information entry, intuitive controls, and readable output of useful information. 

1. Design. UX Deliverable(s): Low and High Fidelity Prototypes
    1. Low-fidelity Prototypes (Wireframe). Illustrate how the content will be laid out on each screen. We will be omitting any aesthetic design details, focusing on creating a visual framework for stakeholders, designers, and developers. This allows all parties to get a feel of how and where content should be placed.
    2. Design Review. The low-fidelity prototype is evaluated against its requirements in order to verify the outcomes of previous activities, and identify issues before committing to - and if need to be re-prioritised - further work.
    3. High-Fidelity Prototypes. These will show all the intended visual and typographic design details of the application, as it would be on final output. This will be handed over to the Engineering team for development.
2. Internal Testing (within Staging). UX Deliverable(s): Adjusted High-fidelity Prototypes
    1. Design QA. To keep the integrity of the user experience, we will conduct Design QA, in collaboration with the Engineering team. This is a step during development, to review the coded version of the UI (prior to testing). This involves working with the Engineers to make updates to the UI in code.
3. Evaluation 
    1. Post-launch Usability Testing. To optimize the application’s usage and the viability of its features, we can conduct a usability test, post-launch.
    2. Quantitative - We’ll look at the analytics of the application, and evaluate the necessary data that correlates to UX/UI pain points, such as the number of errors, number of clicks, or time taken to complete the task. From the data gathered, we’ll come up with suggestions for improvements, and potential next steps.	
    3. Qualitative - We’ll send out a survey, and conduct interviews with the primary and secondary users, to evaluate the features and functionality that can be improved. We’ll propose enhancements that can be made, for future iterations of the application

## 5. Understanding

### 5.1 Data

#### 5.1.1 Existing data sources

As part of the initial engagement, a definition and analysis will be done on the existing state of data sources based on variables such as centralization, accessibility, performance, structure, and more. This will be achieved through what is known as a “data maturity assessment” (DMA), developed by [TDWI](https://tdwi.org/pages/maturity-model/big-data-maturity-model-assessment-tool.aspx), and as a result will signal on the current gaps and opportunities.

The outlined stages respond to the following items:
+ Nascent: the nascent stage represents a pre–big data environment. 
+ Pre-adoption: during the pre-adoption stage, the organization is starting to do its homework regarding big data analytics.
+ Early adoption: this stage of maturity is typically characterized by one or two proofs of concept (POCs) which become more established and production ready.
+ The chasm: as organizations try to move from early adoption to corporate adoption, there is generally a series of hurdles they need to overcome. There is the obvious challenge of obtaining the right skill set. But also might be restrained with business issues such as politics, funding, data management and governance, architecture etc.
+ Corporate adoption: corporate adoption is the major crossover phase in any organization’s big data journey. During corporate adoption, end users typically get involved, gain insights, and transform how they do business.
+ Mature/visionary: at this stage, organizations are executing big data programs in an efficient and effective manner using a highly tuned infrastructure with well-established program and data governance strategies.

As part of the assessment, the following dimensions are evaluated:

Depending on the use case, different dimensions might be more significant, however it is good to go through this assessment to have insight on what the current state is. This uncovers significant details on where to invest time and resources by identifying gaps and understanding the current baseline of data systems.

As part of the assessment a series of questions are completed that are scored 1 to 5 for each answer with up to 50 points allocated to each of the major dimensions. Based on those the maturity assessment can be done. 

The measuring scale is outlined as follows:

When the assessment is complete, you might see scores like this:


#### 5.1.2 New data sources
Following the analysis of existing data sources, a proposal and plan will be developed which will focus on connecting and structuring the existing data sources and also extending and implementing new data sources that are required to enable the development of the proposed solutions.

Based on the data maturity assessment, we can easily identify where gaps lie and how to address those. The major goal is to improve on the current base line and extend the level of data centralization, integration and having more insightful data. As a relevant example here - Dwell times of carts need further insight and data infrastructure to allow for smart decisions and optimizations to be done in order to systematically reduce the time spent dwelling and improve the overall performance of the system.

### 5.2 Architecture

While we are flexible on technology underlying your architecture, our understanding is that you have existing infrastructure in `<Azure, AWS, GCP>`. The Architecture below is a starting point for addressing the implementation of your product.

At the core is the simulator engine, written in `<e.g., Python>` and utilizing `<e.g., AWS Lambda>`. On top are the Command Line Interface (CLI) application and an API that calls the engine. The functionality of the simulator engine will be exposed via the API Management service to both the CLI and Web applications. The CLI application will output JSON or CSV files, for use in other applications.

We are aware of legacy tools for `<legacy products>` built in `<e.g., .Net and SQL server>` that connect directly with `<e.g., IBM Maximo>`. Our approach is to build our tools as APIs (e.g., using `<e.g., Apigee>`) to build a service architecture. This will also enable greater efficiency with third parties.


## 6. Cost estimates

We propose three ways to estimate the costs. 

### 6.1 Time and material estimate

First is the time and material estiamte. Expect that a product team will have between 1 to 10 people at any given that. Anything more than 10 is becoming at risk of too much communication overhead. It is probably wiser to reduce the team size and reduce the set of features. If that's no possible, then consider splitting the product into two products. 

Blended market hourly rates for outsourced people in India, Philippines, or Eastern Europe will range from $20 to $80. If the work is performed on site in US/Canada/Europe, expect to pay $60 to $250. Anything more than that you are most likely paying overhead costs to the staffing agency/consulting company. For instance, it is not unheard of to see hourly rates in the $1,000+ range for partners in prestigious consulting firms (similar to top law firms). 

Our estimated time and material cost is `<estimate>`

### 6.2 Fixed cost estimate

The main advantage of a fixed cost estimate (e.g., borken down by phase) is its simplicity. The main drawback is that building a product is akin to doing research - the outcomes are always uncertain. While the time and material estimate is biased in favor of the people doing the work (because they have no "deadline"), the fixed cost estiamte is biased in favor of the company purchasing the services. 

Our estimated fixed cost is `<estimate>`

### 6.3 Target and cap cost

The contract is based on two figures: the “target price” and the “cap”.  The cap is the maximum that `<company financing the work>` will pay. The target is lower than the cap and the contract gives both parties a financial incentive to meet the target. If we `<provider of work>` come in under the target, the savings are shared equally between both parties.  Likewise, if we come in over the target, the extra cost is shared evenly – but only up to the cap.  If we reach the cap, it acts like a fixed price. The costs listed are target costs. Assume 25% extra for the cap.

### 6.4 Co-location costs

Estimated colocation costs for each team member staying near New Wales is between $2,000 for 1 week, or $4,300 for one month. We include flights (round trip), travel insurance, accommodation, and per diem.

| | For one week For one month |
| - | - |
| Flights | $1,000 | $1,000 |
| Accomodation | $500 | $1,500 |
| Insurance, Lyft/Uber, telecom | $175 | $500 |
| Per diem | $325 | $1,300 |
| Estimated total | $2,000 | $4,300 |

You will be billed actual cost within reasonable deviations. These costs could be reduced, for instance, by leveraging your existing corporate housing, if available.

## 7. Tools and process

### 7.1 QA and testing
Quality assurance, automation and testing are crucial to the process of implementing software with a high degree of quality and reliability. The purpose of such engineers is to provide the necessary tooling, testing and support. This process validates and ensures functioning of software by going through phases such as smoke tests (validating main functionality), comprehensive tests (manual detailed testing and verification for low volume edge cases and mission critical components), and regression tests (validating that older parts of the software still work when the new part is introduced). Finally QA defines the processes and tooling for testing and deployment automation to ensure an auditable release process.	

### 7.2 Cyber security
This is to be completed based on `<company>` Cyber Security requirements/processes (sanitized, best in class examples from other clients are available). The best practice is to leverage the help of an expert consultant who is separate from the development team and provides an audit of the system independent of the implementation. 

### 7.3 Productivity tools
We are open to adopting `<compnay>` preferred/existing set of collaboration tools (e.g., Microsoft Team, GitHub). We recommend `<e.g., BitBucket, JIRA, and Confluence>` for engaging with product and technical teams and `<e.g., Slack>` is recommended for team communication.

### 7.4 Processes
As a general engagement model and base process, an agile approach is taken as the basis. However more traditional models such as waterfall have their benefits and thus the final shape of the process is a hybrid, where quality planning is done on a milestone level usually on a monthly or quarterly basis depending on the scope and length of the project. Meanwhile the biweekly sprints allow for agility and flexibility to shape priorities during the development.

### 7.5 Sprint cadence 
The general engagement model is:
1. Biweekly sprints
2. Weekly meetings
3. Demo at the end of each sprint
4. Frequent email and chat communication

The goal is to demonstrate or deliver:
1. A high degree of agility on a high level
2. To abstract operational details from the client
3. Give `<compnay>`` leadership the ability to steer the process
4. Find the right balance between comfort and control for the client

The MVP has a higher engagement rate in order to work out the nuances of the working relationship for the benefit of long term collaboration. The process can be further shaped on the go as the need arises.

### 7.6 Sprint meetings		
Sprint planning meetings are held every two weeks. The goal of this meeting is formulation of sprint goal and definition of workload for the next sprint. To be more efficient at the sprint planning meeting with the product owner, team members familiarize themselves with top user stories in the product backlog and have internal sprint planning meeting during work hours. After that, they have a meeting with the product owner to confirm sprint workload and clarify requirements if needed. 

Daily status report meetings are 15-minute meetings held everyday with or without the presence of the product owner. The goal of the daily meeting is to make sure that there are no impediments to the progress towards the sprint goal. 

Backlog grooming meetings are organized in collaboration with the product owner, weekly or less frequently. One of the responsibilities of the development team is product backlog refinement and this can be done on dedicated meetings or offline.

Design review meetings are held in the first or second week of each sprint. This is where the development team, product owner, and designers discuss designs and application flow for the next sprint. Tech sessions are held as needed to discuss major milestones and technical decisions.	

