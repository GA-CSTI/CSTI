---
layout: post
title:  "가상현실과 증강 현실"
---

# 2024학년도 1학기


증강 현실은(AR) 디지털 콘텐츠가 현실세계의 경험에 매끄럽게 융합되는 차세대 컴퓨팅플랫폼으로 인식

증강현실은 디지털 컨테츠를 사용하여 우리의 현실 세계를 개선하여 우리의 경험에 더 나은 정보, 이해가치를 추가하는 것

증강현실은 컴퓨터에서 생성된 그래픽이 실제 현실의 시각과 결합되는 시각적 증각과 가장 일반적으러 연관 - 대부분이 시각적이다

ex) 휴대폰, 테블릿, AR안경

AR은 단순한 컴퓨터 그래픽 오버레이가 아니다
- 실제와 가상의 결합, 실시간 결합, 그래픽을 실제 3D로 등록해야 한다.
- HUD(화면상의 광고 같은거)는 AR이 아니다.

가상객체를 3D로 등록하려면 AR 장치가 3D공간에서 위치를 추적하고 장면의 객체를 배치하기 위해 주변 환경을 매핑할 수 있는 기능이 있어야 한다.

위치 및 방향 추적을 위한 여러 기술 및 기술과 다음을 포함한 환경 특징 감지
- Geolocation : GPS는 사용자의 위치에 대한 저해상도 추적을 제공
- Image Tracking : QR 코드 마커, 게임 카드, 제품 포장
- Motion Tracking : SLAM ( 위치 체크하고 매핑하는 기술 )
- Environmental Understanding : 환경에 특징을 감지
- Face and Object Tracking : 증강된 셀카 사진은 카메라를 사용하여 얼굴 마스크 or 이미지 향상에 사용할 수 있는 3D mesh를 감지

유니티 3D 엔진과 AR Foundation툴킷 패키지를 사용하고 있다.
- AR Foundation은 Google, AR Core, Apple ~, Magic Leap 등에서 제공하는 장치별 시스템 기능 위에 장치 독립적 SDK를 제공

===================================================
유니티에서 시작하기

유니티는 여러 개의 대체 렌더링 파이프 라인을 제공
더 새로운 SRP 기반 파이프라인은 더욱 성능이 뛰어나고 유연하다
- 고급 그래픽 하드웨어를 사용한 고품질 렌더링을 위한 HDRP가 포함
- 모바일 장치에서도 매우 빠른 동시에 우수한 렌더링 품질을 제공하는 URP도 있다.

1. XR Plugin Management 설치
2. AR Foundation  + XR Interaction Toolkit
			- Google ARCore XR Plugin 
3. XR Subsystems


AR 비디오 배경 지원을 추가해야 하는 추가 사항이 있다.
- 이 기능은 가상 그래픽이 픽셀 위에 전달 되기 전에 기기기의 비딩가 화면에 바로 공급 되도록 한다.


평면에 사용자의 기능을 추가하여 장면에 3D 가상 객체를 배치
- 화면을 탭 할때 PlaceObject 입력 작업 설정
- 입력 작업에 대응하는 개체를 평면에 배치하는 Plan 스크립트 작성
- AR Raycast Manager를 사용하여 개체를 배치할 평면 및 위치 결정
- InputSystem 이용시 Action을 정의 - > 이거이 더  유용

=================================================== 
유니티 설정

AR Core는 싱글 스레드를 사용하는 OpenGLES3.0만 지원
-    멀티 스레드 기반의 Vulkan은 지원하지 않음

AR Foundation과 AR Core의 경우, 스마트폰의 안드로이드 OS 버전이 7.0 이상( API 레벨 24 이상) 이어야만 사용 가능

AR Session
	- AR Core 시스템을 관리하고 스마트폰의 카메라 장치에 접근(Access)하는 역활
	- 유니티와 별도로 AR 시스템의 라이프사이클(Lifecycle) 처리를 담당
	- 매 프레임마다 카메라로부터 화면 이미지나 위치 수신 여부를 결정하는 AR Core의 핵심 기능을 담당


월드 공간에 배치된 오브젝트와 카메라로 찍은 이미지(화면)의 상대적인 위치를 잡을 수 있어야 함
	세션 공간(Session Space) - 카메라로 찍은 화면이 위치할 공간
	AR Camera - 세션 공간의 기준점으로 사용할 카메라 오브젝트 

XR Origin
	- XR Origin 오브젝트는 스마트폰의 카메라 위치 및 회전을 유니티 월드 공간 좌표로 환산하는 역할 
	- AR Camera를 자식 오브젝트로 보유

AR Plane Mesh Visualizer (AR 카메라에 감지된 바닥면의 형태에 따라 mesh를 동적으로 생성하는 컴포넌트)
- AR Plane 컴포넌트가 만드시 필요 ( AR 카메라 장치가 찍은 화면으로 부터 바닥을 인식하는 역할)

유니티 Raycast 함수는 3D 월드 공간을 기준으로 Ray의 충돌 여부를 측정하는 반면, AR Foundation의 Raycast함수는 스크린의 픽셀을 기준으로 Ray의 충돌을 측정함


FACE


AR Face Manager
	- 스키린으로부터 얼굴을 인식하고 중심 좌표와 인식된 얼굴의 정점 정보, UV 좌표 정보 등을 실시간으로 생성
	- 얼굴을 인식하는 순간마다 Face Prefab 변수에 할당된 게임 오브젝트를 얼굴 중심 좌표에 생성
	- 별도의 설정 없이 자동으로 AR 카메라가 스마트폰의 전면 카메라가 작동

AR Face
	- Face Manager를 통해 Scene에 생성될 때, 지정된 Prefab에 AR Face 컴포넌트가 없으면 자동으로 생성
	- 생성된 Prefab의 AR Face 컴포넌트를 코드상에서 다룰 가능성도 있기 때문에 미리 컴포넌트를 추가

AR Foundation
	- 얼굴을 468개 Vertex로 구성된 Mesh를 자동으로 생성하고, Vertex의 자표나 UV 좌표 등의 데이터에 접근할 수 있으므로 움직이는 얼굴 모델링을 쉽고 빠르게 구현할 수 있음

AR Face Mesh Visualizer
	- 자동으로 AR Face 컴포넌트도 함께 추가
	- AR Face Mesh Visualizer 스크립트에서 AR Face 컴포넌트에 의존적 클래스로 만들었기 때문에

얼굴의 특정 지점의 위치를 가져오는 방법 두가지
	- AR Core API 이용 
	- AR 카메라로 인식된 이마의 좌우 부분과 코 끝의 위치와 회전 값을 Pose 구조체로 전달
	- ARCoreFaceSubsystem - XRFaceSubsystem 클래스를 상속함, 얼굴 데이터를 제어하는 기능을 갖고 있는 변수를 부모 클래스에서 자식 클래스로 캐스팅 해야함
																			- subSys = (ARCoreFaceSubsystem)manager.subsystem;
	- AR Foundation  API 이용
	- Face Mesh를 만들 대 468개 Vertex 데이터의 배열에서 알고 싶은 얼굴 부위에 가장 가까운 정점 좌표를 알아냄
	- XRFaceSubsystem - 디바이스의 카메라 장치와 AR Face Manager 클래스 간의 데이터 처리나 명령 등에 대한 로우 레벨 수준의 함수를 가지고 있음



Marker 인식

Keep Texture at Runtime - 앱실행 중에 이미지의 참조 값을 로드하고 있는 여부(최적화를 위해 체크 안함)

```csharp
public GameObject indicator;
public GameObject myCar;
public float relocationDistance = 1.0f;

ARRaycastManager arManager;
GameObject placedObject = null;

List<ARRaycastHit> hitInfos = new List<ARRaycastHit>();	// Ray에 부딪힌 대상드르이 정보를 저장할 리스트

void Start(){
	indicator.SetActive(false);
	arManager = GetComponent<ARRaycastManager>();
}

void Update(){
	DetectGround();

	if(indicator.activeInHierarchy && Input.touchCount > 0){
		Touch touch = Input.GetTouch(0);
		if(touch.phase == TouchPhase.Began){
			if( placeObject == null)
				placedObject = Instantiate(myCar, indicator.transform.position, indicator.transform.rotation);
			else{
				if(Vector3.DIstance(placedObject.transform.position, indicator.transform.position) > relocationDistance) {
					placedObject.transform.SetPositionAndRotation(indicator.transform.position, indicator.transfomr.rotation);
				}
			}
		}
	}
}
void DetectGround(){
	Vector2 screenSize = new Vector2(Screen.width * 0.5f, Screen.heigh * 0.5f)l
	if( arManager.Raycast(screenSize, hitInfos, TrackavleType.Planes)){
		indicator.SetActive(true);
		indicator.transform.position = hitInfos[0].pose.position;
		indicator.transform.rotation = hiinfos[0].pose.rotation;
	}
		else{
			indicator.setActive(fasle);
	}
```


```csharp
=====================================Face Scripts

UI_Manafer.cs

public Material[] faceMats;

public void ToggleMaskImage(){
	foreach(ARFace face in faceManager.trackables){
		if(face.trackingState == TrackingState.Tracking){
			face.gameObject.SetActive(!face.gameObject.activeInHierarchy);
		}
	}
}		

public void SwitchFaceMaterial(int num){
	foreach(ARFace face in faceManager.trackables){
		if(face.trackingState == TrackingState.Tracking) {
			face.gameObject.GetComponent<MeshRenderer>().material = faceMats[num];
		}
	}
}


FindDetection.cs

public ARFaceManager manager;
public GameObject smallCube;

List<GameObject> testCubes = new List<GameObject>();
ARCoreFaceSubsystem sybSys;
NativeArray<ARCoreFaceRegionData> regionData;

void Start(){
	for(int i=0; i<3; i++){
		GameObject go = Instantiate(smallCube);
		testCubes.Add(go);
		go.SetActive(false);
	}
	manager.facesChanged += OnDetectThreePoints;

	subSys = (ARCoreFaceSubSystem)manager.subsystem.;
}





void OnDetectThreePoints(ARFacesChangedEventArgs args){
	if(args.updated.Count > 0){
		subSys.GetRegionPoses(args.updated[0].trackableId, Allocator.Persistent, ref regionData);

		for(int i=0; i<regionData.Length; i++){
			testCubes[i].transform.position = regionData[i].pose.position;
			testCubes[i].transform.rotation = regionData[i].pose.rotation;
			testCubes[i].SetActive(true);
		}
	}
	else if(args.removed.Count > 0){
		for( int i=0; i<testCubes.Count; i++){
			testCubes[i].SetActive(false);
		}
	}
}


void OnDetectFaceAll(ARFacesChangedEventARgs args){
	if(args.updated.Count > 0 ){
		int num = int.Parse(vertexIndex.text);
		//Vector3 vertexPosition = args.updated[0].vertices[100];
		Vector3 vertexPosition = args.updated[0].vertices[num];
		vertexPosition = args.updated[0].transform.TransformPoint(vertexPosition);
		testCubes[0].SetActive(true);
		testCubes[0].transform.position = vertexPosition;
	}
	else if(args.removed.Count > 0)
		testCubes[0].SetActive(false);
}
```



```csharp
========================================================================= Marker

BallController.cs

public float resetTime = 3.0f;
public float captureRate = 0.3f;
public TMP_Text result ;

Rigidbody rb;
bool isRead = true;
Vector2 startPos;

void Start(){
	result.text = "";
	rb = GetComponent<Rigidbody>();
	rb.isKinematic = true;
}

void Update(){
	if(!isReady)
		return;

	SetBallPosition(Camera.main.transform);
	
	if(Input.touchCount > 0  && isReady){
		Touch touch = Input.GetTouch(0);
		
		if(touch.phase == TouchPhase.Began){
			startPos = touch.position;
		}
		else if( touch.phase == TouchPhase.Ended){
			float dragDistance = touch.position.y - startPos.y;
			Vector3 throwAngle = (Camera.main.transform.forward + Camera.main.transform.up).normalized;

			rb.isKinematic = false;
			isReady = false;

			rb.AddForce(throwAngle * dragDistance * 0.005f, ForceMode.VelocityChange);

			Invoke("ResetBall", resetTime);
		}
	}
}


void SetBallPosition(Transform anchor){
	Vector3 offset = anchor.forward * 0.5f + anchor.up * -0.2f;
	transform.position = anchor.position + offset;
}

void ResetBall(){
	rb.isKinematic = true;
	rb.velocity = Vector3.zero;
	isReady = true;
}

void OnCollisionEnter(Collision collision){
	if(isReady)
		return;
	float draw = Random.Range(0, 1.0f);
	if(draw <=captureRate){
		result.text = "Successful capture!";
	else
		result.text = "You failed and it ran away..."
	Destroy(collision.gameObject);
	gameObject.SetAcive(false);
}




MulitypleImageTracker.cs

ARTrackedImageManager imageManager;

void Start(){
	imageManager = getComponent<ARTrackedImageManager>();
	imageManager.trakedImagesChanged += OnTrackedImage;
}

void OnTrackedImage(ARTrackedImagesChangedEventArgs args){
	foreach(ARTrackedImage trackedImage in args.added){
		string imageName = trackedImage.referenceImage.name;
		GameObject imagePrefab = Resources.Load<GameObject>(imageName);
		if(imagePrefab != null){
			if (trackedImage.transform.childCount <1){
				GameObject go = Instantiate(imagePrefabm trackedImage.transform.position, trackedImage.transform.rotation);
				go.transform.SetParent(trackedImage.transform);
			}
		}
	}
	foreach(ARTrackedImage trackedImage in args.updated){
		if(trackedImage.transfom.childCount >0){
			trackedImage.transform.CetChild(0).position = trackedImage.transform.position;
			trackedImage.transform.GetChild(0).rotation = trackedImage.transform.rotation;
		}
	}
}
```	
	
=====================================================================
3차원 공간의 물체(마커와 증강 물체)

-Tracking ( 위치, 회전 방향)
	- 3차원 공간에서 사용자의 시점을 추적
	- 3차원 공간에서 실제 공간 속의 물체를 추적 ( ex) 마커, 실제 공간의 기하 정보)
	
-Tracking 기번의 3가지 분류
	- 능동형 ( Active )
		- 인위적인 신호를 발생시키고 이를 다시 감지하는 방식 
		- 기계적인 방식 : 관절에 센서가 부착된 기계팔을 움직여서 Tracking ( haptict)
		- 장점 : 높은 정확도,  - 단점 : 관절의 자유도에 따라 움직임 제약, 대부분의 기기가 매우 비쌈
		- 자기장 방식 : 자기장을 발생시킨 후 센서에 감지된 패턴 변화 분석
		- 장점 : 높은 움직임 자유도(6 DOF) , - 단점 : 노이즈 때문에 거리가 멀어질수록 정확도/안정성 하락, 자기장 발생장치의 가격이 비쌈
    		- GPS - 전세계적으로 29개의 GPS 방식의 위성이 운용중 , 삼각측량
				GPS 수신기의 위치 측정 : 4개 이상의 위성에게 신호 수신,각 위성으로 부터의 도착 시간 분석
		- GPS, Wi-Fi , 휴대폰 위치 추적

	- 수동형 ( Passive)
		- 자연에 존재하는 신호를 감지하는 방식 ( 관성 센서 (가속도, 자이로))
		- 관성 센서 ( 가속도계, 자이로센서)
		- 장점 : 인위적인 신호 발생자치 불필요 , 대부분 가격이 저렴, 작은 크기 가능,  고주파수 -> 매우 짧은 시간간격으로 측정 가능
		- 단점 : 떨림(Drift) 현상, 낮은 움직임 자유도(3 DOF)
		- 관성 센서(나침반 센서) - 지구의 자기장을 이용
		- 가속도계 -  X/Y/Z 축 방향으로의 움직임 감지, 기울임 감지, 시간에 따른 떨림 현상
		- 영상 기반 기법 ( 컴퓨터 비젼, 마커 기반 Tracking, Natural Feature Tracking ) 
		- 식별 가능한 표지(Tracker) 부착
		- 컴퓨터 비젼 : 자연속에 존재하는 특징점 필요, 안정성을 해결해야함, 계산량이 많음
		- Marker Tracking - 오랜 시간동안 널리 사용된 방식, 구현하기 쉬움, 대부분 정사각 마커(4개의 모서리 존재)	
		- Marker Tracking
				장점 : 계산량이 적음
				단점 : Tracking이 필요한 곳에 반드시 마커를 배치해야함, 대개의 경우 마커가 온전히 다 확인 되어야 제대로 동작함
		-Markerless Tracking
				장점 : 사용자의 눈에 띄거나 거슬리지 않음, 기본적으로 어디에서나 사용 가능한 기술
				단점 : 막대한 계산량
			- Edge - Based Tracking : 모서리 인식, 특징점을 연결한 선분 추출, 추출된 특징점과 선분을 기반으로 위치/회전 파악
			- Texture Tracking
			- 3D 모델 Tracking : 추적 대상 모델의 기본 정보를 미리 파악 후 추적 ( 형태, 물체의 연결구조, 변형/동작 가능한 한계 )
			- OpenTL : 3D 모델 Tracking에 최적화, 여러 개의 대상을 동시에 추적 가능, 병렬처리와 GPU 가속을 통해 성능 최적화, 플랫폼에 상관 없이 동작
			- Natural Feature Tracking : 현실 세계의 특징부위 추출 (꼭지점, 윤곽선, 표면의 무늬) 
					
	- 혼합형
		- 두 개 이상의 Tracking 기법을 합쳐서 서로를 보완하는 방식
		- GPS( 실외 ) + 마커 기반 Tracking ( 실내 )
		



-Registration
	- 실제 공간 속에 증강 물체를 정합, 증강 하고자 하는 CG 물체를 공간 속에 배치하는 것
	- 고려해야 할 요소 : 사용자의 시점, 주변 환경의 기하 구조, 증강할 CG 물체
	- 사전에 필요한 작업 : Calibration ( 사용하는 카메라의 다양한 매개변수 파악 필요 => 렌즈 왜곡률, 초점거리 )
	- 증강 물체와 현실 환경이 완벽하게 정렬 목표
	- Tracking의 안정성이 Registration의 결과에 직결됨
	- 정렬에 실패( or 오차가 발생) 할 경우 같은 공가에 함께 존재한다는 감각이 붕괴
	- Registration 오류의 원인 2가지
		- 정적인 오류( Static Error ) : 시점이 움직이지 않을 때 발생		
		- 렌즈 왜곡, 기계적인 결함, 잘못 측정된 카메라 매개변수들
		- 줄이는 방법 : 수작업으로 시행착오를 거쳐 매개변수 조절, 렌즈 왜곡률 보정, 정밀한 카메라 Calibration
		- 동적인 오류 ( Dynamic Error ) : 시점이 움직일 때 발생
		- Tracking에 소요되는 계산시간으로 인한 지연 발생, 빠른 움직으로 인해 흐릿해진 영상( Motion Blur )
		-  줄이는 방법 : 시스템 지연시간 줄이기( 더 빠른 부품, 알고리즘 ) , 입력영상의 재생을 의도적으로 늦춤, Predictive Tracking ( 시점의 다음 이동 위치를 예측 ) : 효율적이지만 위험 부담이 있음
	- 여러개의 Tracking 기법을 조합하여 사용하는 것이 안정성 및 효율이 우수함


===============================
증강현실 디스플레이

다양한 방식의 디스플레이들
	- 오디오 디스플레이
	- 햅틱 디스플레이                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
		- 촉각을 기반으로 한 감각을 재현하는 기술
	- 후각 디스플레이

씨쓰루 와 비씨쓰루
씨쓰루( See - Through )
	- 광학( Optical ) 씨쓰루 디스플레이 : 광학 반사경 등을 활용하여 이미지 조합
							현실 세계의 모습  + 컴퓨터로 생성한 가상 이미지 두개가 섞임
	- 비디오 씨쓰루 디스플레이 (Optical 예시) : 카메라로 현실 영상 획득 후 프로세서로 전달
								영상 합성 장치 : 입력받은 현실 영상에서 기하정보 분석, 분석된 기하정보를 토대로 증강 물체를 합성
								이슈 : 시점과 영상의 위상 차이 보정 , 시점과 영상 획득 위치의 차이
	- Occlusion Shadows 기법 : 증강 물체가 생성될 실제 공간을 불투명하게 조절 ( 증강 물체의 색상을 좀 더 선명하게 볼 수 있음 )
	
	- 광학 VS 비디오 씨쓰루 
		품질 : 비디오 씨쓰루가 더 선명히 보인다.
		정합 : 비디오 씨쓰루의 오차가 더 없다.
		밝기 조절 : 광학 - 디스플레이 장치의 밝기에 의존 ,  비디오 씨쓰루 - 밝기나 대비 등을 소프트웨어적으로 조절 가능
		오류 대응 : 비디오 씨쓰루는 화면이 아예 꺼짐
