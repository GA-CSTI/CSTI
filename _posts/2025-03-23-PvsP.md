# 1. 위치 임베딩 vs 위치 인코딩

`-` 위치 임베딩

위치별로 학습 가능한 임베딩을 부여하는 방식 (단어들의 위치에 따라 **학습 가능한** 값을 부여)

- 모델이 학습을 통해 위치별 임베딩 값을 조정할 수 있다.
- 예를 들어, 위치 0번, 1번, 2번, ...에 대해 각각 학습 가능한 벡터가 존재한다.

$PE_{learned}(pos)=Embedding(pos)$

`-` 위치 인코딩 

위치 정보를 사인(sin)과 코사인(cos) 함수로 생성하는 방식이야. (단어들의 위치에 따라 **학습 불가능한** 상수 값을 부여

- 사전 정의된 함수(수학적 패턴)를 사용하기 때문에 변화하지 않는 상수 값
- 학습을 통해 조정되지 않고, 입력 데이터에 대해 고정된 값을 사용한다.

$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$

$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$

- pos는 단어의 위치,
- i는 임베딩 차원의 인덱스
- d는 임베딩 차원 크기

# 2. 각 방식의 장단점

`Good` 위치 임베딩 장점
1. 학습 가능: 위치 임베딩은 학습이 가능하기에 모델이 데이터에 맞춰 위치 정보를 **최적화**할 수 있다.
2. 더 높은 유연성: 모델이 데이터를 통해 학습하므로, 특정 문맥이나 도메인에 맞춰 위치 표현을 다르게 할 수 있다.

`Bad` 위치 임베딩 단점
1. 메모리와 연산 비용: 위치 임베딩은 각 위치마다 임베딩 벡터를 학습해야 하기 때문에, 추가적인 파라미터와 메모리를 사용한다.
2. 오버피팅: 학습 가능한 위치 벡터가 많아지면, 모델이 훈련 데이터에 **과적합**할 가능성이 커질 수 있다.

`Good` 위치 인코딩 장점
1. 메모리 효율성: 파라미터가 없기에 메모리 부담이 적다.
2. 고정된 패턴: 위치 인코딩은 고정된 수학적 패턴을 사용하므로, 특정 위치 간의 관계를 명확하게 정의할 수 있다.

`Bad` 위치 인코딩 단점
1. 학습할 수 없음: 모델이 학습을 통해 위치 정보를 더 정교하게 조정할 수 없다.
2. 유연성 부족: 모든 문장에서 같은 패턴을 사용하기에 특정 도메인이나 데이터 셋에 대해 최적화된 위치 정보를 표현할 수 없다.

# 3. 결론

- 위치 인코딩은 일반적인 용도와 효율성이 중요한 경우에 적합하다.
- 위치 임베딩은 특정 도메인이나 고유한 데이터셋에 대해 더 정교한 위치 표현이 필요한 경우에 유리하다.
