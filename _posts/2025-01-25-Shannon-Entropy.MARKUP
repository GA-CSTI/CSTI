---
layout: single
title:  "Shannon Entropy"
date:   2025-01-25 9:31::00 -0800
---
<p>
    Welcome! In this post, we’ll explore the concept of Shannon Entropy, 
    a fundamental measure of information in probability theory and information science.<br>
</p>
<p style="font-size: 14px">
    If you are not familiar with probability distributions, you may have a quick review on this
    <a href="{{ site.baseurl }}/Probability-Distribution-Quick-Review/">
    probability distribution - quick review</a>.
</p>

<h1>Definition</h1>

<p>Shannon entropy, introduced by Claude Shannon, quantifies the <strong>uncertainty</strong> or <strong>randomness</strong> in a data source. It is a fundamental concept in information theory and measures the average amount of information contained in a random variable.</p>

<h2>Formula for Shannon Entropy</h2>
<p>For a discrete random variable <i>X</i> with <i>n</i> possible outcomes \( \{x_1, x_2, \dots, x_n\} \), each having a probability \( P(x_i) \), the Shannon entropy \( H(X) \) is defined as:</p>

<p>\[
H(X) = -\sum_{i=1}^n P(x_i) \log_b P(x_i)
\]</p>

<ul>
    <li>\( P(x_i) \): Probability of the outcome \( x_i \).</li>
    <li>\( \log_b \): Logarithm with base \( b \) (usually base 2, giving entropy in bits).</li>
    <li>Negative sign ensures that entropy is non-negative since \( P(x_i) \) is between 0 and 1.</li>
</ul>

<h2>Key Intuitions</h2>

<h3>1. Higher Entropy → More Uncertainty</h3>
<p>If all outcomes are equally likely, entropy is maximized because we gain the most information by observing the outcome.</p>

<h3>2. Lower Entropy → Less Uncertainty</h3>
<p>If one outcome is highly likely, entropy is lower because the result is more predictable, so less information is gained.</p>

<h3>3. Zero Entropy</h3>
<p>When the outcome is completely certain (\( P(x_i) = 1 \) for one outcome and 0 for all others), entropy is zero because there is no uncertainty.</p>
<p>The graph below shows how the entropy varies for one probability state as mentioned above.</p>
<img src="/assets/images/probability-vs-entropy.png" alt="Entropy vs. Probability Distribution" style="width:70%;margin:10px 0;">
<h2>Why Logarithms?</h2>
<h3>a) Logarithms Measure Information Content</h3>
<p>The information content \( I(x_i) \) of an outcome \( x_i \) is inversely related to its probability \( P(x_i) \):</p>

<p>\[
I(x_i) = -\log_b P(x_i)
\]</p>
<img src="/assets/images/logarithms-measure-information-content.png" alt="Logarithms measure information content" style="width:70%;margin:10px 0;">
<p>The less likely an event, the more surprising and informative it is.</p>

<h3>b) Logarithms Are Additive</h3>
<p>The property \( \log_b(xy) = \log_b(x) + \log_b(y) \) makes logarithms suitable for combining uncertainties of independent events.</p>

<h3>c) Logarithmic Growth Matches Diminishing Returns</h3>
<p>The logarithmic scale reflects diminishing returns in information gain: observing a rare event adds much more information than observing a common one.</p>

<h2>Why the Negative Sign?</h2>
<p>Probabilities \( P(x_i) \) are between 0 and 1, so \( \log_b(P(x_i)) \) is negative. The negative sign ensures entropy is positive.</p>

<h2>Examples</h2>

<h3>1. Fair Coin Flip</h3>
<p style="font-size: 16px">\[
H(X) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ bit}
\]</p>

<h3>2. Biased Coin Flip (90% Heads, 10% Tails)</h3>
<p style="font-size: 16px">\[
H(X) = - (0.9 \log_2 0.9 + 0.1 \log_2 0.1) \approx 0.469 \text{ bits}
\]</p>

<h3>3. Uniform Distribution Over 4 Outcomes</h3>
<p style="font-size: 16px">If \( P(x_i) = 0.25 \):</p>
<p style="font-size: 16px">\[
H(X) = - (4 \cdot 0.25 \log_2 0.25) = 2 \text{ bits}
\]</p>
<h2>Explanation of Entropy Differences</h2>

<h3>1. Fair Coin Flip (\(H(X) = 1 \text{ bit}\))</h3>
<p>In a <strong>fair coin flip</strong>, there is equal probability (\(P(\text{Heads}) = 0.5\)) of landing heads or tails.</p>
<img src="/assets/images/fair-coin-flip-histogram.png" alt="Fair coin flip histogram" style="width:70%;margin:10px 0;">
<p>Since both outcomes are equally likely, there is <strong>maximum uncertainty</strong>; we can't predict the outcome better than pure chance.</p>
<img src="/assets/images/fair-coin-flip-entropy.png" alt="Fair coin flip entropy" style="width:70%;margin:10px 0;"></p>
<p><strong>Entropy is highest</strong> because there is no bias or preference in the distribution. Each flip carries 1 bit of information.</p>

<h3>2. Biased Coin Flip (\(H(X) \approx 0.469 \text{ bits}\))</h3>
<p>In this case, one outcome (Heads) is much more likely (\(P(\text{Heads}) = 0.9\)) than the other (\(P(\text{Tails}) = 0.1\)).
<img src="/assets/images/biased-coin-flip-histogram.png" alt="Biased coin flip histogram" style="width:70%;margin:10px 0;"></p>
<p>The <strong>bias reduces uncertainty</strong> because the outcome is more predictable. We can reasonably guess "Heads" most of the time with confidence.</p>
<img src="/assets/images/biased-coin-flip-entropy.png" alt="Biased coin flip entropy" style="width:70%;margin:10px 0;">
<p><strong>Entropy is lower</strong> compared to the fair coin flip because the distribution is uneven,  <span style="color: red;">making the system less random</span>.</p>

<h5>3. Uniform Distribution Over 4 Outcomes (\(H(X) = 2 \text{ bits}\))</h5>
<p>Here, there are four outcomes, each with an equal probability (\(P(x_i) = 0.25\)).<br>
<img src="/assets/images/four-outcomes-histogram.png" alt="Four outcomes histogram" style="width:70%;margin:10px 0;"></p>
<p>The <strong>uncertainty is higher</strong> than in the coin flip examples because there are more possible outcomes. Predicting any one outcome is more difficult since each is equally likely.</p>
<img src="/assets/images/four-outcomes-entropy.png" alt="Four outcomes entropy" style="width:70%;margin:10px 0;">
<p><strong>Entropy is maximized</strong> for this distribution (given 4 outcomes), as it represents the most random scenario for a uniform probability over four options.</p>

<h3>Key Takeaways</h3>
<ul>
    <li>Entropy increases with the number of possible outcomes and is maximized for uniform distributions.</li>
    <li>Entropy decreases when the distribution becomes skewed or biased because predictability increases and randomness decreases.</li>
</ul>

<h2>Why This Formula Works</h2>
<ul>
    <li><strong>Non-Negativity:</strong> \( H(X) \geq 0 \), with equality when \( P(x_i) = 1 \) for some \( i \).</li>
    <li><strong>Maximal Entropy for Uniform Distributions:</strong> Entropy is maximized when all outcomes are equally likely.</li>
    <li><strong>Additivity:</strong> For independent events, \( H(X, Y) = H(X) + H(Y) \).</li>
    <li><strong>Continuity:</strong> Small changes in probabilities result in small changes in entropy.</li>
</ul>

<h2>Conclusion</h2>
<p>The Shannon entropy formula elegantly captures our intuitive understanding of uncertainty and information. It is fundamental to data compression, cryptography, machine learning, and more.</p>
