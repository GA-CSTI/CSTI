---
title: "[NLP] About Transformers"
author: "차상진"
date: "2025-03-20"
---
# Transformers

## 1. 입력 데이터 전처리

- 임베딩 레이어: 입력 데이터를 고정된 차원의 연속형 벡터로 변환합니다.
- 위치 인코딩: 단어의 순서를 반영하기 위해 사인 및 코사인 함수를 이용한 위치 인코딩을 추가한다.
- 최종 입력: 입력 임베딩과 위치 인코딩을 더한 값을 인코더에 전달

## 2. 인코더 구조 (여러 개의 인코더 레이어 스택)

- 인코더는 여러 개의 인코더 레이어로 이루어지며, 각 인코더 레이어는 다음을 포함한다.
1. 멀티-헤드 셀프 어텐션 (Multi-Head Self-Attention)
    - 입력 문장에서 각 단어가 문맥을 반영해 서로 다른 중요도를 가질 수 있도록 어텐션을 적용합니다.
    - (입력 임베딩 + 위치 인코딩) -> 쿼리(Q), 키(K), 밸류(V) 생성 -> 어텐션 결과 출력
2. 잔차 연결 (Residual Connection) + Layer Normalization
    - 입력 값(입력 임베딩 + 위치 인코딩)과 어텐션 결과를 더한 후 정규화 적용 (**이상하다고 생각 가능 -> 밑에서 설명**)
3. 피드포워드 네트워크 (Feedforward Network, FFN)
    - 비선형 변환을 수행하는 두 개의 완전연결층(FC)으로 구성된다.
    - 비선형 변환이란 ReLU와 같은 활성화 함수를 의미한다.
    - **비선형 변환을 이용하는 이유는 학습의 다양성과 안정성을 높이기 위해서이다. 선형 변환만으로는 곱셈과 덧셈으로만 나타내기에 함수의 형태가 단순한데 비선형 변환을 통해 더 복잡한 함수와 관계를 학습할 수 있다.**
4. 잔차 연결 + Layer Normalization
    - FFN의 출력을 다시 입력과 더한 후 정규화합니다. (2번과 비슷하게 입력 값을 다시 더해준다)
    - 그 후 정규화를 한다. 즉, `LayerNorm(FFN Output + Attention Output)`
      
- 이러한 인코더 레이어를 여러 개 쌓아 깊은 표현 학습을 가능하게 합니다.

`-` 이상한 부분의 보충설명

    Q. 어텐션 결과라는 것은 입력값을 멀티-헤드 셀프 어텐션을 거쳐서 나온 결과잖아요. 그런데 그 결과랑 입력 값을 또 더한다구요? 왜 또 더해요?

    A. 어텐션의 출력은 입력을 바탕으로 생성된 값인데, 다시 입력과 더하는 것이 이상하게 보일 수 있음. 그 이유는 잔차 연결(Residual Connection)**은 신경망이 깊어질수록 발생하는 학습 어려움을 해결하기 위해 도입된 기법입니다. 만약 어텐션 출력만 다음 레이어로 넘긴다면, 원래 입력 정보가 손실될 수 있음. 따라서, 원래 입력 정보를 유지하면서도, 어텐션이 학습한 새로운 정보(출력)를 추가적으로 반영하기 위해 입력 + 어텐션 출력을 더함.

    잔차 연결이 없다면? : 인코더/디코더가 깊어질수록 입력 정보가 왜곡될 가능성이 커진다. 그러므로 입력 정보(원래 값) + 어텐션 결과(새로운 정보)를 함께 유지하는 것이 학습 안정성 측면에서 유리함. 이것이 잔차 연결의 핵심 개념이다.

## 3. 디코더 구조 (여러 개의 디코더 레이어 스택)

- 디코더도 여러 개의 디코더 레이어로 구성되며, 인코더와 다른 점은 인코더-디코더 어텐션 레이어가 추가된다는 것입니다.
- 각 디코더 레이어는 다음을 포함합니다.
1. 마스크드 멀티-헤드 셀프 어텐션 (Masked Multi-Head Self-Attention)
    - 디코더는 정답 데이터를 예측하는 과정에서 앞쪽 단어만 참고할 수 있도록 마스크를 적용하여 미래 정보를 차단합니다.
    - 미래 정보(미래에 나올 단어)를 미리 안다면 정답지를 보고 컨닝을 하는 것과 같기에 올바론 학습이 되
  지 않는다.
    - 마스크드 멀티 헤드 어텐션도 인코더의 멀티 헤드 어텐션과 똑같이 Q,K,V 값 계산 -> 어텐션 결과 출력 
2. 잔차 연결 + Layer Normalization
    - 인코더 구조에서 설명한 것과 같음.
3. 인코더-디코더 어텐션 (Encoder-Decoder Attention)
    - 인코더에서 출력된 벡터를 사용하여 입력 문장과 현재 디코더 상태 간의 관계를 학습합니다.
    - 디코더만 사용하는 모델(GPT)는 이 층을 사용할 수 없다. 인코더에서 나온 출력을 이용하기에 인코더와 디코더가 모두 있는 모델에서만 이 층을 사용한다.
4. 잔차 연결 + Layer Normalization
    - 3에서 추가된 인코더-디코더 어텐션 층이 있기에 잔차 연결 + Layer Normalizaiton 층이 추가된다.
6. 피드포워드 네트워크 (FFN)
7. 잔차 연결 + Layer Normalization

## 4. 출력 처리

- 선형 변환 (Linear Layer): 디코더에서 나온 출력을 단어 집합 크기만큼의 차원으로 변환합니다.
- 소프트맥스 (Softmax): 확률 분포를 구해 최종적으로 가장 확률이 높은 단어를 예측합니다.

## 요약 (순서 정리)
1. 입력 데이터 처리 (임베딩 + 위치 인코딩)
2. 인코더 (여러 개의 인코더 레이어)
    -멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화 → FFN → 잔차 연결 + 정규화
3. 디코더 (여러 개의 디코더 레이어)
    -마스크드 멀티-헤드 셀프 어텐션 → 잔차 연결 + 정규화
    -인코더-디코더 어텐션 → 잔차 연결 + 정규화
    -FFN → 잔차 연결 + 정규화
4. 출력 변환 (선형 레이어 → 소프트맥스 → 예측)
